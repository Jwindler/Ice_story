# Transformers 发展一览

Transformers 研究概览

![](https://s2.loli.net/2023/04/22/ON2sLu6nm4CixZB.png)



## 1. 介绍

近年来，深度学习的研究步伐显着加快，因此越来越难以跟上所有最新发展。尽管如此，有一个特定的研究方向因其在自然语言处理、计算机视觉和音频处理等多个领域取得的成功而备受关注。这在很大程度上归功于其高度适应性的架构。该模型称为 Transformer，它利用了该领域的一系列机制和技术（即注意力机制）。



## 2. 分类

迄今为止，基于 vanilla Transformer 探索了一系列全面的模型，大致可分为三类：

- 网络结构修改
- 预训练方法
- 应用

![](https://s2.loli.net/2023/04/22/kiNZqyUzVWlw6MT.png)



上面的每个类别都包含其他几个子类别，我将在接下来的部分中对其进行彻底研究。图 2. 说明了研究人员修改 Transformers 的类别。



## 3. 注意力

自注意力在 Transformer 中起着基本作用，尽管它在实践中有两个主要缺点。

1. 复杂性：对于长序列，该模块成为瓶颈，因为其计算复杂度为 O(T²·D)。
2. 结构先验：它不解决输入的结构偏差，需要将额外的机制注入训练数据，稍后它可以学习（即学习输入序列的顺序信息）。

![](https://s2.loli.net/2023/04/22/HLw4hp2kI5Rv1Os.png)



因此，研究人员探索了各种技术来克服这些缺点。

1. 稀疏注意力：该技术试图通过考虑输入的一小部分而不是整个输入序列来降低注意力机制的计算时间和内存要求，从而生成与完整矩阵相反的稀疏矩阵。
2. 线性化注意力：使用内核特征映射解开注意力矩阵，该方法试图以相反的顺序计算注意力，以将资源需求降低到线性复杂度。
3. 原型和内存压缩：这一行修改试图减少查询和键值对，以实现更小的注意力矩阵，从而减少时间和计算复杂度。
4. Low-rank self-attention：通过使用参数化或用低秩近似替换它来显式地建模自注意力矩阵的低秩属性，试图提高 transformer 的性能。
5. 先验注意力：利用其他来源的先验注意力分布，这种方法将其他注意力分布与从输入中获得的注意力分布相结合。
6. 改进的多头机构：有多种方法可以修改和提高多头机构的性能，可以归入该研究方向。



## 4. 总结

总之，Transformer 的分类学和注意力机制的各种进步显着扩展了基于 Transformer 的模型的能力和效率。稀疏注意力技术，例如基于位置和基于内容的稀疏注意力，以及线性化注意力，已经解决了传统密集注意力的计算局限性。查询原型和内存压缩方法引入了创新的方法来提高注意力机制的效率。低秩自注意力启用了参数化和近似技术，以实现更有效的注意力计算。结合先验，例如局部性建模、较低的模块先验和多任务适配器，已经在改善注意力机制方面显示出可喜的结果。最后，对多头机制的修改，例如头部行为建模、限制跨度、精细聚合和其他变体，显示出进一步提高基于 Transformer 的模型性能的潜力。

注意机制的这些进步为未来在自然语言处理、计算机视觉和机器翻译等各个领域的研究和应用提供了令人兴奋的前景。通过利用这些创新技术，基于变压器的模型可以继续突破性能和效率的界限，为高级机器学习应用开辟新的可能性。