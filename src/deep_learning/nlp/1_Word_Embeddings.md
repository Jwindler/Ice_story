# 自然语言处理：词嵌入简介



## Word Embeddings

机器学习模型“查看”数据的方式与我们（人类）的方式不同。例如，我们可以轻松理解“我看到一只猫”这一文本，但我们的模型却不能——它们需要特征向量。此类向量或词嵌入是可以输入模型的词的表示。

![](https://s2.loli.net/2023/03/26/hBkQHucl2b8IgsJ.png)



工作原理：查找表（词汇）

![](https://s2.loli.net/2023/03/26/yzDlVSa5WJ1jY2P.png)



在实践中，你有一个允许单词的词汇表；你提前选择这个词汇。对于每个词汇单词，查找表包含它的嵌入。可以使用词汇表中的单词索引找到该嵌入（即，您可以使用单词索引在表中查找嵌入）。

![](https://s2.loli.net/2023/03/26/Z3K4tUfyXkibMvu.png)



为了解释未知词（那些不在词汇表中的词），通常一个词汇表包含一个特殊的标记 UNK。或者，未知标记可以被忽略或分配一个零向量。

[本讲](https://lena-voita.github.io/nlp_course/word_embeddings.html "Source")的主要问题是：我们如何得到这些词向量？



## 表示为离散符号：One-hot 向量

最简单的方法是将单词表示为One-hot向量：对于词汇表中的第 i 个单词，向量在第 i 个维度上为 1，在其余维度上为 0。在机器学习中，这是表示分类特征的最简单方法。

![](https://s2.loli.net/2023/03/26/dN5kLFBstiP6cTE.png)



您可能会猜到为什么One-hot向量不是表示单词的最佳方式。问题之一是对于大词汇表，这些向量会很长：向量维数等于词汇表大小。这在实践中是不可取的，但这不是最关键的问题。

真正重要的是，这些向量对它们所代表的词一无所知。例如，One-hot向量“认为”猫和狗的距离和桌子的距离一样近！我们可以说 **one-hot 向量不捕获意义**。

但是我们怎么知道什么是意义呢？



## 分布语义

为了在向量中捕捉单词的含义，我们首先需要定义可以在实践中使用的含义概念。为此，让我们尝试了解我们人类如何知道哪些词具有相似的含义。

![](https://s2.loli.net/2023/03/26/uVAftBCsXMb2Er3.png)



一旦您看到了未知词在不同上下文中的使用方式，您就能够理解它的含义。你是怎么做到的？

假设是你的大脑搜索了可以在相同上下文中使用的其他词，找到了一些（例如，葡萄酒），并得出了 tezgüino 与其他词具有相似含义的结论。这是分布假设：

> 经常出现在相似上下文中的词具有相似的含义。

这是一个非常有价值的想法：它可以在实践中使用，让词向量捕捉到它们的含义。根据分布假设，“捕捉意义”和“捕捉上下文”在本质上是相同的。因此，我们需要做的就是将有关单词上下文的信息放入单词表示中。

> 主要思想：我们需要将有关单词上下文的信息放入单词表示中。



## 基于计数的方法

![](https://s2.loli.net/2023/03/26/2hmeTjDMuFdpP8A.png)



基于计数的方法从字面上理解了这个想法：

> 如何：根据全球语料库统计信息手动放置此信息。

一般过程如上图所示，包括两个步骤：(1) 构建词上下文矩阵，(2) 降低其维数。降维有两个原因。首先，原始矩阵非常大。其次，由于很多单词只出现在少数几种可能的上下文中，因此该矩阵可能包含很多无信息的元素（例如，零）。

要估计词/上下文之间的相似性，通常需要评估归一化词/上下文向量的点积（即余弦相似性）。

要定义基于计数的方法，我们需要定义两件事：

![](https://s2.loli.net/2023/03/26/6jFpKXRQYor9ydD.png)



1. 可能的上下文（包括一个词出现在上下文中意味着什么）
2. 关联的概念，即计算矩阵元素的公式



### Co-Occurence Counts

![](https://s2.loli.net/2023/03/26/7Tk8rt4MRgOVhFY.png)



最简单的方法是将上下文定义为 L 大小窗口中的每个单词。词-上下文对 (w, c) 的矩阵元素是 w 在上下文 c 中出现的次数。这是获取嵌入的非常基本（而且非常非常古老）的方法。



### Positive Pointwise Mutual Information (PPMI)

![](https://s2.loli.net/2023/03/26/IkFUaMKE4fst2bh.png)



这里上下文的定义和之前一样，但是单词和上下文之间关联的度量更加巧妙：positive PMI（或简称 PPMI）。 PPMI 度量被广泛认为是前神经分布相似性模型的最新技术。



### 潜在语义分析 (LSA)：理解文档

![](https://s2.loli.net/2023/03/26/PjAVKqNbaYEI8DZ.png)



潜在语义分析 (LSA) 分析一组文档。虽然在之前的方法中上下文仅用于获取词向量并随后被丢弃，但在这里我们也对上下文感兴趣，或者在本例中是文档向量。 LSA是最简单的主题模型之一：文档向量之间的余弦相似度可以用来衡量文档之间的相似度。

术语“LSA”有时指的是将 SVD 应用于术语文档矩阵的更通用方法，其中术语文档元素可以用不同的方式计算（例如，简单的共现、tf-idf 或其他一些权重） 