# 探讨|使用或不使用机器学习

机器学习擅长解决某些复杂问题，通常涉及特征和结果之间的困难关系，这些关系不能轻易地硬编码为启发式或 if-else 语句。然而，在决定 ML 是否是当前给定问题的良好解决方案时，有一些限制或需要注意的事项。在[这篇文章](https://towardsdatascience.com/to-use-or-not-to-use-machine-learning-d28185382c14 "Source")中，我们将深入探讨“使用或不使用 ML”这一主题，首先了解“传统”ML 模型，然后讨论随着生成式 AI 的进步，这种情况将如何变化。

为了澄清一些观点，我将使用以下举措作为示例：“作为一家公司，我想知道我的客户是否满意以及不满意的主要原因”。解决这个问题的“传统”基于机器学习的方法可能是：

- 获取客户对您的评论（应用程序或 Play 商店、Twitter 或其他社交网络、您的网站……）
- 使用情感分析模型将评论分为正面/中性/负面。
- 对预测的“负面情绪”评论使用主题建模来了解它们的含义。

![](https://s2.loli.net/2023/08/05/RfDp4oAhLEtwV8F.png)



## 数据有足够的质量和数量吗？

在监督 ML 模型中，训练数据对于模型学习需要预测的任何内容（在本例中为评论中的情绪）是必要的。如果数据质量低（大量错别字、缺失数据、错误……），模型就很难表现良好。

> 这通常被称为“垃圾输入，垃圾输出”问题：如果您的数据是垃圾，那么您的模型和预测也将是垃圾。

同样，您需要有足够的数据量供模型学习影响需要预测的不同因素。在此示例中，如果您只有负面评论标签，其中包含“无用”、“失望”或类似概念，则模型将无法了解到这些词通常在标签为“负面”时出现。

足够数量的训练数据还应该有助于确保您能够很好地表示执行预测所需的数据。例如，如果您的训练数据无法代表特定地理区域或特定人群，则模型更有可能在预测时无法很好地处理这些评论。

对于某些用例，拥有足够的历史数据也很重要，以确保我们能够计算相关的滞后特征或标签（例如“客户是否在明年支付信用额”）。



## 标签定义是否清晰且易于获取？

同样，对于传统的监督机器学习模型，您需要一个带标签的数据集：您知道想要预测的最终结果的示例，以便能够训练您的模型。

标签的定义是关键。在此示例中，我们的标签将是与评论相关的情绪。我们可能认为我们只能发表“正面”或“负面”评论，然后认为我们也可能发表“中立”评论。在这种情况下，根据给定的评论，通常会清楚标签是否需要是“正面”、“中立”或“负面”。但是想象一下，我们有“非常积极”、“积极”、“中立”、“消极”或“非常消极”的标签……对于给定的评论，是否很容易决定它是“积极”还是“非常积极” ”？需要避免标签缺乏明确的定义，因为使用嘈杂的标签进行训练将使模型更难学习。

现在标签的定义已经很清楚了，我们需要能够获得足够的、高质量的示例集的标签，这些示例将形成我们的训练数据。在我们的示例中，我们可以考虑手动标记一组评论，无论是在公司还是团队内部，还是将标记外部化给专业注释者（是的，有人全职为 ML 标记数据集！）。需要考虑与获得这些标签相关的成本和可行性。

![](https://s2.loli.net/2023/08/05/NrDfcKwTYBQ7VIR.png)



## 解决方案的部署是否可行？

为了达到最终效果，机器学习模型的预测需要可用。根据用例，使用预测可能需要特定的基础设施（例如 ML 平台）和专家（例如 ML 工程师）。

在我们的示例中，由于我们希望将模型用于分析目的，因此我们可以离线运行它，并且利用预测将非常简单。然而，如果我们想在负面评论发布后 5 分钟内自动做出回应，那就另当别论了：需要部署和集成模型才能实现这一点。总的来说，重要的是要清楚地了解使用预测的要求是什么，以确保在可用的团队和工具的情况下它是可行的。



## 有什么利害关系？

机器学习模型的预测总会存在一定程度的误差。事实上，ML 中有一句经典的话：

> 如果模型没有错误，那么数据或模型肯定有问题

理解这一点很重要，因为如果用例不允许这些错误发生，那么使用 ML 可能不是一个好主意。在我们的示例中，想象一下，我们使用该模型将客户的电子邮件分类为“是否提出指控”，而不是评论和情绪。拥有一个可以对对公司提出指控的电子邮件进行错误分类的模型并不是一个好主意，因为这可能会给公司带来可怕的后果。



## 使用机器学习在道德上是否正确？

已经有许多经过验证的预测模型基于性别、种族和其他敏感个人属性进行歧视的案例。因此，机器学习团队需要谨慎对待他们在项目中使用的数据和功能，同时也要质疑从道德角度来看，自动化某些类型的决策是否真的有意义。您可以查看我之前关于该主题的博客文章以了解更多详细信息。



## 我需要可解释性吗？

机器学习模型在某种程度上就像一个黑匣子：你输入一些信息，它们就会神奇地输出预测。模型背后的复杂性就是这个黑匣子背后的原因，特别是当我们与统计中的简单算法进行比较时。在我们的示例中，我们可能无法准确理解为什么评论被预测为“正面”或“负面”。

在其他用例中，可解释性可能是必须的。例如，在保险或银行等受到严格监管的行业。银行需要能够解释为什么向某人授予（或不授予）信贷，即使该决定是基于评分预测模型的。

这个话题与伦理道德有着密切的关系：如果我们不能完全理解模型的决策，就很难知道模型是否已经学会了歧视。



## 这一切会因为生成人工智能而改变吗？

随着生成式人工智能的进步，许多公司正在提供网页和 API 来使用强大的模型。这如何改变我之前提到的有关 ML 的限制和考虑因素？

- 数据相关主题（质量、数量和标签）：对于可以利用现有 GenAI 模型的用例，这肯定会发生变化。大量数据已用于训练 GenAI 模型。这些模型中的大多数都没有控制数据的质量，但这似乎弥补了它们使用的大量数据。由于这些模型，我们可能不再需要训练数据（同样，对于非常具体的用例）。这被称为零样本学习（例如“询问 ChatGPT 给定评论的情绪是什么”）和少样本学习（例如“向 ChatGPT 提供一些正面、中立和负面评论的示例，然后要求其提供对新评论的看法”）。关于这一点的一个很好的解释可以在 deeplearning.ai 时事通讯中找到。
- 部署可行性：对于可以利用现有 GenAI 模型的用例，部署变得更加容易，因为许多公司和工具正在为这些强大的模型提供易于使用的 API。如果出于隐私原因需要对这些模型进行微调或将其引入内部，那么部署当然会变得更加困难。

![](https://s2.loli.net/2023/08/05/c7MU6LyG2W9jrts.png)



无论是否利用 GenAI，其他限制或考虑因素都不会改变：

- 高风险：这将继续成为一个问题，因为 GenAI 模型的预测也存在一定程度的错误。谁没有见过 GhatGPT 产生幻觉或提供毫无意义的答案？更糟糕的是，评估这些模型变得更加困难，因为无论其准确性如何，响应听起来总是充满信心，并且评估变得主观（例如“这个响应对我有意义吗？”）。
- 道德：仍然像以前一样重要。有证据表明 GenAI 模型可能会因用于训练的输入数据而产生偏差（链接）。随着越来越多的公司和功能开始使用这些类型的模型，明确这可能带来的风险非常重要。
- 可解释性：由于 GenAI 模型比“传统”机器学习更大、更复杂，其预测的可解释性变得更加困难。目前正在进行研究来了解如何实现这种可解释性，但它仍然非常不成熟（链接）。



## 总结

在这篇博文中，我们了解了在决定是否使用 ML 时需要考虑的主要事项，以及随着生成式 AI 模型的进展，情况会发生怎样的变化。讨论的主要主题是数据的质量和数量、标签获取、部署、风险、道德和可解释性。我希望这个总结在您考虑下一个 ML 计划（或不考虑）时有用！